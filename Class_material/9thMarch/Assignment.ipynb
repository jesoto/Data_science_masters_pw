{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with\n",
    "an example.\n",
    "\n",
    "- Probability Mass Function (PMF):\n",
    "\n",
    "    The PMF is used for discrete random variables, which are variables that can only takes on a finite or countably infinite number of values. It defines the probabilitu of each possible value of the random varaible. The PMF gives the probability of observing a specific value, and it satisfies two properties: all probabilities are non negative and the sum of all probabilities is equal to 1.\n",
    "     \n",
    "    Example:\n",
    "\n",
    "    Lets consider a fair six sided die. The possible values that can ve obtained are 1,2,3,4,5,6 each with an equal probabilitu of 1/6. The PMF of this random variable would assign a probability of 1/6 ot each of these values, and the PMF would be defined as follows:\n",
    "\n",
    "        PMF(1) = 1/6        \n",
    "        PMF(2) = 1/6\n",
    "        PMF(3) = 1/6\n",
    "        PMF(4) = 1/6\n",
    "        PMF(5) = 1/6\n",
    "        PMF(6) = 1/6\n",
    "\n",
    "\n",
    "- Probability Density Function (PDF):\n",
    "\n",
    "    The PDF is used for continuous random variables, which are variables that can take on any value within a certain range. The PDF defines the probability distribution by specifying the likelihood of the random variable falling within a particular interval. Unlike the PMF, the PDF itself does not give the probability of a specific value since the probability of any single point in a continuous distribution is technically zero. Instead, the PDF represents the relative likelihood of different outcomes.\n",
    "\n",
    "    Example: Let's consider the height of adult males in a population. The height is a continuous random variable that can take on any value within a range (e.g., between 150 cm and 200 cm). The PDF would describe the likelihood of observing different heights. For simplicity, let's assume that the distribution follows a normal distribution. The PDF would take the shape of a bell curve, with the highest point at the mean height.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?\n",
    "\n",
    "- The cumulative density function is a concept used in probability theory and statistics to describe the cumulative probabilitu distribution of a random varaible. It gives the probabilitu that a random variable takes on a value less than or equal to a given value.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    Lets consider a fair six sided die. The CDF of this random variable would indicate the probability of obtaining a value less than or equal to a given value. For each possible value, the cdf qpuld increase by the probability associated with that value.\n",
    "\n",
    "\n",
    "    CDF(1) = 1/6\n",
    "    CDF(2) = 2/6\n",
    "    CDF(3) = 3/6\n",
    "    CDF(4) = 4/6\n",
    "    CDF(5) = 5/6\n",
    "    CDF(6) = 1\n",
    "    \n",
    "- Why is CDF used?\n",
    "    The CDF is useful for several reasons:\n",
    "\n",
    "    - Probability Calculation: The CDF allows us to calculate the probability of a random variable taking on a value less than or equal to a given value. This is particularly helpful in determining percentiles or quantiles of a distribution.\n",
    "\n",
    "    - Distribution Comparison: The CDF can be used to compare different probability distributions. By comparing the CDFs of two random variables, we can assess which distribution has a higher probability of smaller or larger values.\n",
    "\n",
    "    - Random Variable Analysis: The CDF provides a comprehensive summary of the probability distribution of a random variable. It allows us to understand the overall behavior of the variable and analyze its characteristics, such as the median, quartiles, and range.\n",
    "\n",
    "    - Random Variable Transformations: The CDF is also essential for transforming random variables. By applying the inverse of the CDF to a random variable uniformly distributed between 0 and 1 (known as a uniform random variable), we can generate random values that follow a desired probability distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What are some examples of situations where the normal distribution might be used as a model?\n",
    "Explain how the parameters of the normal distribution relate to the shape of the distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal distribution, also known as the Gaussian distribution, is a probability distribution that is widely used to model various phenomena in many fields. Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "- Physical measurements: In many cases, when we measure physical quantities such as height, weight, or blood pressure in a population, the data tends to follow a normal distribution. This allows us to use the normal distribution to describe and analyze the variability in these measurements.\n",
    "\n",
    "- IQ scores: Intelligence quotient (IQ) scores are often modeled using the normal distribution. The distribution of IQ scores in the general population is approximately normal, with the majority of individuals falling around the average score and fewer individuals having scores that deviate significantly from the mean.\n",
    "\n",
    "- Errors in measurements: When making measurements or conducting experiments, there is often some degree of random error involved. Assuming that the errors are normally distributed is a common practice, and it allows for the use of statistical methods that rely on the properties of the normal distribution.\n",
    "\n",
    "- Financial markets: In finance, stock returns and other financial variables are often modeled using the normal distribution. Although the assumption of normality may not always hold in reality, it provides a useful approximation for many purposes, such as risk management and option pricing.\n",
    "\n",
    "Now, let's discuss how the parameters of the normal distribution relate to its shape. The normal distribution is defined by two parameters: the mean (μ) and the standard deviation (σ).\n",
    "\n",
    "- Mean (μ): The mean represents the central location or the average value of the distribution. It determines the position of the peak or center of the distribution. If the mean is shifted to the right, the entire distribution is shifted to the right, and if it is shifted to the left, the distribution is shifted to the left.\n",
    "\n",
    "- Standard deviation (σ): The standard deviation measures the spread or variability of the distribution. It determines the width of the distribution. A smaller standard deviation indicates that the data points are clustered closely around the mean, resulting in a narrower distribution. Conversely, a larger standard deviation indicates a wider spread of data points, resulting in a broader distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explain the importance of Normal Distribution. Give a few real-life examples of Normal\n",
    "Distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal distribution holds significant importance in various fields due to its numerous desirable properties and wide applicability. Here are a few reasons why the normal distribution is important:\n",
    "\n",
    "1. Central limit theorem: The central limit theorem states that the sum or average of a large number of independent and identically distributed random variables tends to follow a normal distribution, regardless of the shape of the original distribution. This theorem is crucial in statistics because it allows us to make inferences and draw conclusions about population parameters based on sample data.\n",
    "\n",
    "2. Data analysis and inference: The normal distribution serves as a fundamental tool for data analysis and inference. Many statistical techniques, such as hypothesis testing, confidence intervals, and regression analysis, rely on the assumption of normality. These methods allow us to draw meaningful conclusions, make predictions, and understand the relationships between variables in a wide range of real-life situations.\n",
    "\n",
    "3. Standardization and comparison: The normal distribution has a standardized form known as the standard normal distribution, with a mean of 0 and a standard deviation of 1. This standardization enables easy comparison and interpretation of data across different domains. By transforming data into standard scores or z-scores, we can determine how unusual or extreme a particular observation is within a given population.\n",
    "\n",
    "4. Risk management and decision-making: Many risk models and decision-making frameworks assume that variables follow a normal distribution. For example, in finance, stock returns and asset prices are often assumed to be normally distributed when estimating risk measures and constructing investment portfolios. Normal distribution also plays a vital role in quality control processes, where deviations from expected values are assessed using control charts based on the normal distribution.\n",
    "\n",
    "Real-life examples of situations where the normal distribution is commonly observed include:\n",
    "\n",
    "- Heights and weights of individuals in a population.\n",
    "- IQ scores and standardized test scores.\n",
    "- Errors in scientific measurements and experimental data.\n",
    "- Blood pressure and other physiological measurements.\n",
    "- Annual rainfall or temperature data.\n",
    "- Stock market returns and financial asset prices (often as an approximation).\n",
    "- Exam scores and grades in educational settings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli\n",
    "Distribution and Binomial Distribution?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli distribution is a discrete probability distribution that models a random experiment with two possible outcomes: success (usually denoted as 1) and failure (usually denoted as 0). It is named after Jacob Bernoulli, a Swiss mathematician. The Bernoulli distribution is characterized by a single parameter, p, which represents the probability of success.\n",
    "\n",
    "Mathematically, the Bernoulli distribution can be described as follows:\n",
    "\n",
    "P(X = 1) = p\n",
    "\n",
    "P(X = 0) = 1 - p\n",
    "\n",
    "where X is the random variable representing the outcome of the experiment.\n",
    "\n",
    "An example of the Bernoulli distribution is flipping a fair coin. Suppose we define success as getting a head and failure as getting a tail. In this case, the probability of success (getting a head) is p = 0.5, and the probability of failure (getting a tail) is 1 - p = 0.5. The outcome of each coin flip follows a Bernoulli distribution.\n",
    "\n",
    "Now, let's discuss the difference between the Bernoulli distribution and the binomial distribution:\n",
    "\n",
    "- Number of trials: The Bernoulli distribution models a single trial or experiment with two possible outcomes. In contrast, the binomial distribution models the number of successes in a fixed number of independent Bernoulli trials.\n",
    "\n",
    "- Parameters: The Bernoulli distribution has a single parameter, p, which represents the probability of success in a single trial. The binomial distribution has two parameters: n, the number of trials, and p, the probability of success in each trial.\n",
    "\n",
    "- Discrete vs. discrete/countinuous: The Bernoulli distribution is a discrete distribution since it only takes on two possible outcomes. The binomial distribution is also discrete, but it represents the number of successes, which can take on multiple discrete values.\n",
    "\n",
    "- Probability mass function vs. probability mass function or probability density function: The Bernoulli distribution is described by a probability mass function (PMF) since it assigns probabilities to discrete outcomes. The binomial distribution is also described by a PMF, which gives the probabilities of obtaining a specific number of successes. However, when the number of trials is large, the binomial distribution can be approximated by a probability density function (PDF) due to the continuity correction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset\n",
    "is normally distributed, what is the probability that a randomly selected observation will be greater\n",
    "than 60? Use the appropriate formula and show your calculations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the probability that a randomly selected observation from a normally distributed dataset with a mean of 50 and a standard deviation of 10 will be greater than 60, we need to use the standard normal distribution and the z-score formula. The z-score measures how many standard deviations an observation is away from the mean.\n",
    "\n",
    "The formula to calculate the z-score is:\n",
    "z = (x - μ) / σ\n",
    "\n",
    "Where:\n",
    "\n",
    "x is the value we want to find the probability for (in this case, 60),\n",
    "μ is the mean of the distribution (50), and\n",
    "σ is the standard deviation of the distribution (10).\n",
    "Let's calculate the z-score:\n",
    "z = (60 - 50) / 10\n",
    "z = 10 / 10\n",
    "z = 1\n",
    "\n",
    "Next, we need to find the probability corresponding to the z-score using a standard normal distribution table or a statistical software. The probability can also be calculated using the cumulative distribution function (CDF) of the standard normal distribution.\n",
    "\n",
    "The probability that a randomly selected observation from the dataset will be greater than 60 can be found by calculating the area under the curve to the right of the z-score of 1. This is equivalent to finding 1 minus the cumulative probability up to the z-score of 1.\n",
    "\n",
    "Using a standard normal distribution table or a statistical software, we can find that the cumulative probability up to a z-score of 1 is approximately 0.8413. Therefore, the probability that a randomly selected observation will be greater than 60 is:\n",
    "\n",
    "P(X > 60) = 1 - 0.8413 = 0.1587\n",
    "\n",
    "So, the probability is approximately 0.1587 or 15.87%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Explain uniform Distribution with an example."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniform distribution is a continuous probability distribution where all outcomes within a given range are equally likely. It is often referred to as the rectangular distribution due to its rectangular shape. In a uniform distribution, the probability density function (PDF) remains constant over the entire range of possible values.\n",
    "\n",
    "Mathematically, the uniform distribution is defined by two parameters: a and b, representing the lower and upper bounds of the distribution, respectively. The PDF of a uniform distribution is given by:\n",
    "\n",
    "f(x) = 1 / (b - a) for a ≤ x ≤ b\n",
    "f(x) = 0 otherwise\n",
    "\n",
    "Here's an example to help illustrate the uniform distribution:\n",
    "\n",
    "Suppose we have a fair six-sided die, where the numbers 1, 2, 3, 4, 5, and 6 are equally likely outcomes. In this case, the uniform distribution can be used to model the probabilities of rolling each number.\n",
    "\n",
    "Here, the lower bound (a) is 1, representing the minimum value on the die, and the upper bound (b) is 6, representing the maximum value on the die. Since each number has an equal chance of occurring, the probability of rolling any specific number is 1/6.\n",
    "\n",
    "Using the uniform distribution formula, the PDF for this example would be:\n",
    "f(x) = 1 / (6 - 1) = 1/5 for 1 ≤ x ≤ 6\n",
    "f(x) = 0 otherwise\n",
    "\n",
    "In this case, the PDF remains constant at 1/5 across the range of possible values (1 to 6) and is zero outside that range.\n",
    "\n",
    "The uniform distribution is not limited to dice rolls; it can also be applied in various real-life scenarios. For instance:\n",
    "\n",
    "- Random number generation: The uniform distribution is often used in random number generation algorithms to generate random values within a specified range.\n",
    "\n",
    "- Random sampling: In statistical sampling, the uniform distribution can be used to select samples randomly from a population when each individual has an equal chance of being chosen.\n",
    "\n",
    "- Resource allocation: When resources need to be allocated fairly among a set of options or individuals, the uniform distribution can be used to ensure an equal probability for each choice.\n",
    "\n",
    "- Simulation modeling: In computer simulations, the uniform distribution is often used to model uncertainty when the data or outcomes are assumed to be uniformly distributed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What is the z score? State the importance of the z score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z-score, also known as the standard score or standard normal deviate, is a measure that indicates how many standard deviations an individual data point is away from the mean of a distribution. It standardizes data by transforming it into a common scale, allowing for meaningful comparisons and analysis across different datasets or distributions.\n",
    "\n",
    "Mathematically, the z-score of a data point (x) in a distribution with mean (μ) and standard deviation (σ) is calculated as follows:\n",
    "\n",
    "z = (x - μ) / σ\n",
    "\n",
    "The z-score tells us how many standard deviations an observation is above or below the mean. A positive z-score indicates that the data point is above the mean, while a negative z-score indicates that it is below the mean. A z-score of 0 means the data point is equal to the mean.\n",
    "\n",
    "The importance of the z-score lies in several key aspects:\n",
    "\n",
    "- Standardization: The z-score standardizes data, allowing for direct comparisons between different datasets or distributions. By transforming data into z-scores, we can assess how each data point relates to the mean and how it compares to other data points within the same dataset or population.\n",
    "\n",
    "- Relative position: The z-score provides information about the relative position of a data point within a distribution. It tells us whether an observation is relatively close to the mean or if it deviates significantly from the average.\n",
    "\n",
    "- Probability calculation: The z-score is used to calculate probabilities associated with a specific value or range of values in a normal distribution. By referring to a standard normal distribution table or using statistical software, we can determine the probability of obtaining a value above or below a certain threshold or within a given range.\n",
    "\n",
    "- Outlier detection: The z-score helps identify outliers in a dataset. Observations with z-scores that are significantly larger or smaller than the majority of the data points may be considered outliers. This can be useful for identifying unusual or extreme values that could be influential or indicative of errors in the data.\n",
    "\n",
    "- Hypothesis testing and confidence intervals: In statistical hypothesis testing, the z-score is often used to calculate test statistics and assess the significance of results. Additionally, in constructing confidence intervals, the z-score is employed to determine the margin of error based on the desired confidence level."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is Central Limit Theorem? State the significance of the Central Limit Theorem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental concept in statistics that states that the sampling distribution of the mean of a sufficiently large number of independent and identically distributed (i.i.d.) random variables will be approximately normally distributed, regardless of the shape of the original population distribution. In simpler terms, it states that when we take a large enough sample size from any population, the distribution of the sample means will approach a normal distribution.\n",
    "\n",
    "The significance of the Central Limit Theorem is profound and has several important implications:\n",
    "\n",
    "- Sampling variability: The CLT provides a theoretical foundation for understanding the behavior of sample means. It explains why, regardless of the shape of the population distribution, the means of random samples tend to cluster around the population mean. This concept is crucial in statistical inference, where we draw conclusions about population parameters based on sample data.\n",
    "\n",
    "- Approximation of the normal distribution: The CLT allows us to approximate the distribution of sample means by a normal distribution. This is extremely useful since the properties of the normal distribution are well understood, making it easier to perform statistical analysis and make probabilistic statements about sample means.\n",
    "\n",
    "- Estimation and confidence intervals: The CLT is central to the construction of confidence intervals and estimation of population parameters. With the knowledge that the distribution of sample means is approximately normal, we can estimate population means and construct confidence intervals around these estimates.\n",
    "\n",
    "- Hypothesis testing: The CLT enables the use of parametric hypothesis tests, such as the t-test and z-test. These tests rely on the assumption that sample means are normally distributed. By applying the CLT, we can assess the significance of results and make statistical decisions based on sample means.\n",
    "\n",
    "- Practical applications: The CLT has wide-ranging applications across various fields. It allows us to make accurate predictions and draw conclusions about population characteristics from random samples. It is particularly relevant in areas such as survey sampling, quality control, econometrics, finance, and experimental design."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. State the assumptions of the Central Limit Theorem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) relies on certain assumptions for its applicability. These assumptions are as follows:\n",
    "\n",
    "- Independence: The random variables being sampled should be independent of each other. This means that the value of one observation does not influence the value of another observation.\n",
    "\n",
    "- Identical distribution: The random variables being sampled should be identically distributed. This means that they should have the same probability distribution, regardless of the specific values observed.\n",
    "\n",
    "- Finite variance: The random variables should have a finite variance. This assumption ensures that the variance of the sample means remains finite as the sample size increases.\n",
    "\n",
    "- Sample size: The CLT assumes that the sample size is sufficiently large. While there is no strict threshold for what constitutes a \"sufficiently large\" sample size, a commonly suggested guideline is that the sample size should be at least 30.\n",
    "\n",
    "It is important to note that the CLT is a theoretical result that holds under these assumptions. Violation of these assumptions may result in the CLT not holding or requiring modifications. In practice, if the sample size is not large or the variables being sampled do not satisfy the assumptions, alternative methods or techniques may need to be employed for accurate statistical inference."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
